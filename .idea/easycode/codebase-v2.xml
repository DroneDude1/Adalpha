<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="com.obiscr.chatgpt.settings.EasyCodeState">
    <option name="projectFiles" value="$PROJECT_DIR$/Adalpha.py;C:/Users/MaClemetsen/OneDrive - Davidson Academy/Documents/AIML/Bike_Dataset/gan_test.py;C:/Users/MaClemetsen/OneDrive - Davidson Academy/Documents/AIML/Bike_Dataset/main.py;C:/Users/MaClemetsen/OneDrive - Davidson Academy/Documents/AIML/Bike_Dataset/tests.py;C:/Users/MaClemetsen/OneDrive - Davidson Academy/Documents/AIML/Bike_Dataset/tests_core.py;C:/Users/MaClemetsen/OneDrive - Davidson Academy/Documents/AIML/Bike_Dataset/utils.py" />
    <option name="forceFullIndex" value="false" />
    <option name="fileSummaryMaps" value="{&quot;C:/Users/MaClemetsen/OneDrive - Davidson Academy/Documents/AIML/Bike_Dataset&quot;:&quot;{\&quot;/main.py\&quot;:\&quot;This main.py file is testing the Adalpha optimization algorithm on the MNIST dataset.\\n\\nThe key things it does:\\n\\n- Imports the test functions from a tests.py module\\n- Imports the Adalpha (AA) module\\n- Runs the main code if this file is executed directly\\n- Calls the mnist_multiple_test function from tests.py\\n- Passes in the Adalpha_Plot and AdAlpha_Momentum functions from AA as the optimizers to test\\n- Sets hyperparameters like epochs, learning rate, chaos punishment, EMA weight for testing\\n\\nKey functions:\\n\\n- mnist_multiple_test - Runs multiple tests (Adalpha_Plot and AdAlpha_Momentum) on MNIST dataset\\n- Adalpha_Plot - The Adalpha optimization algorithm with plotting enabled  \\n- AdAlpha_Momentum - Adalpha with momentum added\\n- AA module - Contains the Adalpha optimization algorithm implementation\\n\\nSo in summary, it is using the tests.py module to run multiple tests of the Adalpha optimizer on MNIST, with different hyperparameters, to evaluate its performance.\&quot;,\&quot;/utils.py\&quot;:\&quot;This file contains various utility functions for data preprocessing and analysis.\\n\\nThe key functions are:\\n\\n- csv_to_data: Reads a CSV file and returns the data as a NumPy array, optionally including column names. Allows filtering rows using a validator function.\\n\\n- homo_csv: Standardizes the formatting of a CSV file by replacing tabs with spaces and multiple spaces with single spaces. \\n\\n- r2_score: Calculates the R-squared value to evaluate regression model performance.\\n\\n- min_max_norm: Normalizes data to the range [0,1] using min-max scaling. \\n\\n- make_date: Converts date strings to NumPy datetime format.\\n\\n- make_heatmap: Creates a 2D heatmap from two 1D arrays by counting co-occurrences. \\n\\n- String_Verifier: Defines a reusable validator class that can standardize string values in data.\\n\\nSo in summary, it provides common data preprocessing, analysis and validation functions for working with CSV data and evaluating machine learning models. The functions help prepare and analyze data in a standardized way.\&quot;,\&quot;/tests.py\&quot;:\&quot;This file contains test functions for evaluating Adalpha (adaptive learning rate method with chaos) on various datasets compared to standard optimizers like Adam.\\n\\nThe key functions are:\\n\\n- bike_test: Tests Adalpha vs Adam on the Seoul bike dataset, returns R-squared scores. \\n\\n- mnist_test: Tests Adalpha vs Adam on MNIST, returns accuracy scores.\\n\\n- bike_multiple_test: Runs bike_test multiple times and collects results.\\n\\n- mnist_multiple_test: Runs mnist_test multiple times and collects results. \\n\\n- bike_chaos_test: Tests Adalpha on bike dataset with different chaos punishment values, graphs results.\\n\\n- mnist_chaos_test: Tests Adalpha on MNIST with different chaos punishment values, graphs results.\\n\\n- cifar_test: Tests Adalpha vs Adam on CIFAR-10 dataset, graphs loss and evaluates models. \\n\\nThe main purpose of the file is to evaluate the performance of Adalpha compared to standard optimizers on different datasets, and also analyze how the chaos punishment parameter affects Adalpha\\u0027s performance. It contains utilities for running multiple trials of the tests and collecting/visualizing the results.\&quot;,\&quot;/gan_test.py\&quot;:\&quot;This Python file contains code for training a reinforcement learning agent to solve the CartPole problem in the OpenAI Gym environment.\\n\\nKey functions:\\n\\n- RLAgent: The main RL agent class that contains the neural network model, memory, and learning logic.\\n\\n- train(): The main training loop that runs multiple episodes of simulation, collects data, and trains the agent.\\n\\n- RLAgent.get_action(): Gets the action for a given state, either randomly exploring or using the neural network. \\n\\n- RLAgent.learn(): Trains the neural network model on sampled data from the replay memory. Implements Q-learning.\\n\\n- RLAgent.store_sim_results(): Stores state, action, reward, next state tuples from simulations. \\n\\n- calc_reward(): Calculates the reward function for the CartPole environment.\\n\\n- MaxExpLayer: A custom Keras layer for element-wise exponentiation, used in the neural network model.\\n\\n- learn_test(): Function to test different optimizers on the RL problem for comparison.\\n\\nSo in summary, it defines an RL agent class with a neural network, trains it on the CartPole environment using Q-learning and experience replay, and includes some testing functions.\&quot;,\&quot;/Adalpha.py\&quot;:\&quot;This file defines an Adalpha optimizer for TensorFlow Keras models.\\n\\nKey things it does:\\n\\n- Defines a MaxAdam base optimizer class that the Adalpha optimizer inherits from. This handles things like building optimizer variables, updating steps, etc.\\n\\n- Defines an AdAlpha_Momentum optimizer class that inherits from MaxAdam. This implements the core Adalpha logic:\\n  - Adjusting the learning rate alpha based on the loss standard deviation passed to the update_loss method\\n  - Activating the momentum and velocity values to increase convergence\\n\\n- Defines MaxAdamCallback and Adalpha_Plot callback classes. These are used to:\\n  - Track and update the loss standard deviation in the optimizer\\n  - Optionally plot the learning rate adjustment over time\\n\\nKey functions:\\n\\n- update_loss() - Updates the loss standard deviation in the optimizer\\n- _m_activ() - Activates the momentum values \\n- update_step() - Performs a parameter update step with Adalpha logic\\n- _calculate_loss_std() - Calculates new loss std and updates optimizer\\n- on_train_end() - Plots results for the Adalpha_Plot callback\\n\\nSo in summary, it defines an Adalpha optimizer class that adjusts the learning rate and momentum dynamically during training based on loss statistics, with callbacks to track this for optional plotting.\&quot;,\&quot;/tests_core.py\&quot;:\&quot;This file contains tests for training models using different optimizers on bike sharing and MNIST datasets.\\n\\nThe key functions are:\\n\\n- adam_train_bike: Trains a bike sharing model using the Adam optimizer and returns metrics.\\n\\n- adalpha_train_bike: Trains a bike sharing model using the Adalpha optimizer. Takes in callback and optimizer functions. Returns metrics. \\n\\n- adalpha_new_train_bike: Similar to adalpha_train_bike but with a different data splitting approach.\\n\\n- adam_train_mnist: Trains a MNIST classifier using Adam and returns metrics.\\n\\n- adalpha_train_mnist: Trains a MNIST classifier using Adalpha. Takes in callback and optimizer functions. Returns metrics.\\n\\nSo in summary, it contains test functions for training models on two different datasets using Adam and a custom Adalpha optimizer. The Adalpha tests allow passing in the optimizer and callback functions to test different implementations. It returns metrics to evaluate and compare the model training performance between the optimizers.\&quot;}&quot;}" />
  </component>
</project>